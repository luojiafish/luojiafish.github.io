{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6608559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 手动计算 BPTT（标量两步 RNN）=====\n",
      "a1 = 1.0000, h1 = tanh(a1) = 0.7616, y1 = 0.7616, l1 = 0.2900\n",
      "a2 = 1.7616, h2 = tanh(a2) = 0.9427, y2 = 0.9427, l2 = 0.4443\n",
      "Total Loss L = 0.7343\n",
      "\n",
      "== 输出层梯度 ==\n",
      "e1 = y1 - y1_hat = 0.7616\n",
      "e2 = y2 - y2_hat = 0.9427\n",
      "dL/dh1 (仅来自 l1) = 0.7616\n",
      "dL/dh2 = 0.9427\n",
      "\n",
      "== t = 2 反向 ==\n",
      "dh2/da2 = 1 - h2^2 = 0.1114\n",
      "dL/da2 = dL/dh2 * dh2/da2 = 0.1050\n",
      "dL/dw_x (来自 t=2) = 0.1050\n",
      "dL/dw_h (来自 t=2) = 0.0799\n",
      "dL/dh1 (来自 t=2)  = 0.1050\n",
      "\n",
      "== t = 1 反向 ==\n",
      "dL/dh1_total = dL/dh1_from_l1 + dL/dh1_from_t2 = 0.8666\n",
      "dh1/da1 = 1 - h1^2 = 0.4200\n",
      "dL/da1 = dL/dh1_total * dh1/da1 = 0.3639\n",
      "dL/dw_x (来自 t=1) = 0.3639\n",
      "dL/dw_h (来自 t=1) = 0.0000\n",
      "\n",
      "== 最终参数梯度（手动） ==\n",
      "dL/dw_x = 0.4689\n",
      "dL/dw_h = 0.0799\n",
      "dL/dw_y = 1.4687\n",
      "\n",
      "===== 用 MXNet autograd 验证 =====\n",
      "MXNet forward: L = 0.7343\n",
      "dL/dw_x (mxnet) = 0.4689\n",
      "dL/dw_h (mxnet) = 0.0799\n",
      "dL/dw_y (mxnet) = 1.4687\n",
      "\n",
      "===== 对比 手算 vs MXNet autograd =====\n",
      "w_x: manual = 0.4689, mxnet = 0.4689\n",
      "w_h: manual = 0.0799, mxnet = 0.0799\n",
      "w_y: manual = 1.4687, mxnet = 1.4687\n"
     ]
    }
   ],
   "source": [
    "# bptt_scalar_mxnet.py\n",
    "# 一个最简单的一维 RNN，两步序列：\n",
    "# 1）用纯 Python 手动推导 BPTT 梯度\n",
    "# 2）用 MXNet autograd 求梯度并对比\n",
    "\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "\n",
    "\n",
    "def forward_and_backward_manual():\n",
    "    print(\"===== 手动计算 BPTT（标量两步 RNN）=====\")\n",
    "\n",
    "    # 参数\n",
    "    w_x = 1.0\n",
    "    w_h = 1.0\n",
    "    w_y = 1.0\n",
    "\n",
    "    # 输入与标签\n",
    "    x1, x2 = 1.0, 1.0\n",
    "    y1_hat, y2_hat = 0.0, 0.0\n",
    "\n",
    "    # 初始隐状态\n",
    "    h0 = 0.0\n",
    "\n",
    "    # ========= 正向传播 =========\n",
    "    # t = 1\n",
    "    a1 = w_x * x1 + w_h * h0\n",
    "    h1 = math.tanh(a1)\n",
    "    y1 = w_y * h1\n",
    "    l1 = 0.5 * (y1 - y1_hat) ** 2\n",
    "\n",
    "    # t = 2\n",
    "    a2 = w_x * x2 + w_h * h1\n",
    "    h2 = math.tanh(a2)\n",
    "    y2 = w_y * h2\n",
    "    l2 = 0.5 * (y2 - y2_hat) ** 2\n",
    "\n",
    "    L = l1 + l2\n",
    "\n",
    "    print(f\"a1 = {a1:.4f}, h1 = tanh(a1) = {h1:.4f}, y1 = {y1:.4f}, l1 = {l1:.4f}\")\n",
    "    print(f\"a2 = {a2:.4f}, h2 = tanh(a2) = {h2:.4f}, y2 = {y2:.4f}, l2 = {l2:.4f}\")\n",
    "    print(f\"Total Loss L = {L:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # ========= 反向传播 =========\n",
    "    # 输出层梯度：dl/dy = y - y_hat\n",
    "    e1 = y1 - y1_hat\n",
    "    e2 = y2 - y2_hat\n",
    "\n",
    "    # 对 h 的梯度（来自当前步的损失）\n",
    "    dL_dh1_from_l1 = e1 * w_y\n",
    "    dL_dh2 = e2 * w_y  # t=2 没有未来步\n",
    "\n",
    "    print(\"== 输出层梯度 ==\")\n",
    "    print(f\"e1 = y1 - y1_hat = {e1:.4f}\")\n",
    "    print(f\"e2 = y2 - y2_hat = {e2:.4f}\")\n",
    "    print(f\"dL/dh1 (仅来自 l1) = {dL_dh1_from_l1:.4f}\")\n",
    "    print(f\"dL/dh2 = {dL_dh2:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # ---- t = 2: h2 -> a2 -> (w_x, w_h, h1) ----\n",
    "    dh2_da2 = 1.0 - h2 ** 2\n",
    "    dL_da2 = dL_dh2 * dh2_da2\n",
    "\n",
    "    # a2 = w_x * x2 + w_h * h1\n",
    "    da2_dw_x = x2\n",
    "    da2_dw_h = h1\n",
    "    da2_dh1 = w_h\n",
    "\n",
    "    dL_dw_x_t2 = dL_da2 * da2_dw_x\n",
    "    dL_dw_h_t2 = dL_da2 * da2_dw_h\n",
    "    dL_dh1_from_t2 = dL_da2 * da2_dh1\n",
    "\n",
    "    print(\"== t = 2 反向 ==\")\n",
    "    print(f\"dh2/da2 = 1 - h2^2 = {dh2_da2:.4f}\")\n",
    "    print(f\"dL/da2 = dL/dh2 * dh2/da2 = {dL_da2:.4f}\")\n",
    "    print(f\"dL/dw_x (来自 t=2) = {dL_dw_x_t2:.4f}\")\n",
    "    print(f\"dL/dw_h (来自 t=2) = {dL_dw_h_t2:.4f}\")\n",
    "    print(f\"dL/dh1 (来自 t=2)  = {dL_dh1_from_t2:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # ---- t = 1: 累加来自 l1 和来自 t2 的梯度 ----\n",
    "    dL_dh1_total = dL_dh1_from_l1 + dL_dh1_from_t2\n",
    "\n",
    "    dh1_da1 = 1.0 - h1 ** 2\n",
    "    dL_da1 = dL_dh1_total * dh1_da1\n",
    "\n",
    "    # a1 = w_x * x1 + w_h * h0 (h0 = 0)\n",
    "    da1_dw_x = x1\n",
    "    da1_dw_h = h0\n",
    "\n",
    "    dL_dw_x_t1 = dL_da1 * da1_dw_x\n",
    "    dL_dw_h_t1 = dL_da1 * da1_dw_h  # 为 0\n",
    "\n",
    "    print(\"== t = 1 反向 ==\")\n",
    "    print(f\"dL/dh1_total = dL/dh1_from_l1 + dL/dh1_from_t2 = {dL_dh1_total:.4f}\")\n",
    "    print(f\"dh1/da1 = 1 - h1^2 = {dh1_da1:.4f}\")\n",
    "    print(f\"dL/da1 = dL/dh1_total * dh1/da1 = {dL_da1:.4f}\")\n",
    "    print(f\"dL/dw_x (来自 t=1) = {dL_dw_x_t1:.4f}\")\n",
    "    print(f\"dL/dw_h (来自 t=1) = {dL_dw_h_t1:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # ---- 汇总参数梯度（对所有时间步求和）----\n",
    "    dL_dw_x = dL_dw_x_t1 + dL_dw_x_t2\n",
    "    dL_dw_h = dL_dw_h_t1 + dL_dw_h_t2\n",
    "\n",
    "    # 输出层参数 w_y 的梯度 = Σ (y_t - y_hat_t) * h_t\n",
    "    dL_dw_y = e1 * h1 + e2 * h2\n",
    "\n",
    "    print(\"== 最终参数梯度（手动） ==\")\n",
    "    print(f\"dL/dw_x = {dL_dw_x:.4f}\")\n",
    "    print(f\"dL/dw_h = {dL_dw_h:.4f}\")\n",
    "    print(f\"dL/dw_y = {dL_dw_y:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return {\n",
    "        \"w_x\": dL_dw_x,\n",
    "        \"w_h\": dL_dw_h,\n",
    "        \"w_y\": dL_dw_y,\n",
    "        \"L\": L,\n",
    "    }\n",
    "\n",
    "\n",
    "def forward_and_backward_mxnet():\n",
    "    print(\"===== 用 MXNet autograd 验证 =====\")\n",
    "\n",
    "    ctx = mx.cpu()\n",
    "\n",
    "    # 参数：标量 NDArray，并 attach_grad\n",
    "    w_x = nd.array([1.0], ctx=ctx)\n",
    "    w_h = nd.array([1.0], ctx=ctx)\n",
    "    w_y = nd.array([1.0], ctx=ctx)\n",
    "    for p in (w_x, w_h, w_y):\n",
    "        p.attach_grad()\n",
    "\n",
    "    # 输入与标签\n",
    "    x1 = nd.array([1.0], ctx=ctx)\n",
    "    x2 = nd.array([1.0], ctx=ctx)\n",
    "    y1_hat = nd.array([0.0], ctx=ctx)\n",
    "    y2_hat = nd.array([0.0], ctx=ctx)\n",
    "    h0 = nd.array([0.0], ctx=ctx)\n",
    "\n",
    "    with autograd.record():\n",
    "        # t = 1\n",
    "        a1 = w_x * x1 + w_h * h0\n",
    "        h1 = nd.tanh(a1)\n",
    "        y1 = w_y * h1\n",
    "        l1 = 0.5 * (y1 - y1_hat) ** 2\n",
    "\n",
    "        # t = 2\n",
    "        a2 = w_x * x2 + w_h * h1\n",
    "        h2 = nd.tanh(a2)\n",
    "        y2 = w_y * h2\n",
    "        l2 = 0.5 * (y2 - y2_hat) ** 2\n",
    "\n",
    "        L = l1 + l2\n",
    "\n",
    "    print(f\"MXNet forward: L = {L.asscalar():.4f}\")\n",
    "\n",
    "    # 反向传播\n",
    "    L.backward()\n",
    "\n",
    "    print(f\"dL/dw_x (mxnet) = {w_x.grad.asscalar():.4f}\")\n",
    "    print(f\"dL/dw_h (mxnet) = {w_h.grad.asscalar():.4f}\")\n",
    "    print(f\"dL/dw_y (mxnet) = {w_y.grad.asscalar():.4f}\")\n",
    "    print()\n",
    "\n",
    "    return {\n",
    "        \"w_x\": w_x.grad.asscalar(),\n",
    "        \"w_h\": w_h.grad.asscalar(),\n",
    "        \"w_y\": w_y.grad.asscalar(),\n",
    "        \"L\": L.asscalar(),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    manual = forward_and_backward_manual()\n",
    "    mxnet_grads = forward_and_backward_mxnet()\n",
    "\n",
    "    print(\"===== 对比 手算 vs MXNet autograd =====\")\n",
    "    for k in [\"w_x\", \"w_h\", \"w_y\"]:\n",
    "        print(f\"{k}: manual = {manual[k]:.4f}, mxnet = {mxnet_grads[k]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
