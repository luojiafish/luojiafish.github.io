{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303da2b8",
   "metadata": {},
   "source": [
    "BPTT 原理 + 为什么对 (h_t) 求偏导 + 直观解释\n",
    "---\n",
    "\n",
    "# 通过时间反向传播（BPTT）原理与推导（从标量链式法则视角）\n",
    "\n",
    "## 1. BPTT 的核心思想\n",
    "\n",
    "给定 RNN 结构：\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(w_x x_t + w_h h_{t-1}),\\qquad\n",
    "y_t = w_y h_t,\\qquad\n",
    "l_t = \\frac12 (y_t - \\hat{y}_t)^2\n",
    "$$\n",
    "\n",
    "总损失为各时间步损失之和：\n",
    "\n",
    "$$\n",
    "L = \\sum_{t=1}^T l_t\n",
    "$$\n",
    "\n",
    "由于某些中间量（如 (h_t)）出现在多个时间步的计算路径中，因此对它们的梯度需要**把所有路径的贡献加起来**：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_t}\n",
    "= \\sum_{\\text{所有经过 } h_t \\text{ 通往 } L \\text{ 的路径}}\n",
    "\\frac{\\partial L}{\\partial h_t}\\Big|_{\\text{该路径}}\n",
    "$$\n",
    "\n",
    "这本质上就是：\n",
    "\n",
    "> **“哪里有自变量，沿哪条路链式求导，并把所有路径相加。”**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 为什么要对 (h_t) 求偏导？它明明不是参数！\n",
    "\n",
    "虽然我们不更新隐状态 (h_t)，\n",
    "但为了对参数（如 (w_x, w_h, w_y)）求梯度，必须使用链式法则：\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_x}\n",
    "= \\sum_t\n",
    "\\frac{\\partial L}{\\partial a_t}\n",
    "\\cdot \\frac{\\partial a_t}{\\partial w_x}\n",
    "$$\n",
    "\n",
    "而\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_t}\n",
    "= \\frac{\\partial L}{\\partial h_t}\n",
    "\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "于是 $(\\frac{\\partial L}{\\partial h_t})$ 就是**计算参数梯度所必需的“中转站梯度”**。\n",
    "它不是用来更新 (h_t)，而是用来继续将梯度往前传播。\n",
    "\n",
    "总结一句话：\n",
    "\n",
    "> **对 (h_t) 求偏导不是为了优化 (h_t)，\n",
    "> 而是为了让梯度能沿着计算图继续传到真正的参数。\n",
    "> (h_t) 在反向传播中只是“梯度高速公路的中间收费站”。**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 标量 RNN 示例：两步序列，从头算到尾\n",
    "\n",
    "我们构造一个简单的一维 RNN：\n",
    "\n",
    "* 参数：\n",
    "  (w_x = 1,\\ w_h = 1,\\ w_y = 1)\n",
    "* 输入：\n",
    "  (x_1 = 1,\\ x_2 = 1)\n",
    "* 目标输出：\n",
    "  (\\hat y_1=0,\\ \\hat y_2=0)\n",
    "* 初始状态：\n",
    "  (h_0 = 0)\n",
    "\n",
    "### 3.1 正向传播\n",
    "\n",
    "第一步：\n",
    "\n",
    "$$\n",
    "a_1 = 1,\\quad\n",
    "h_1 = \\tanh(1)=0.7616\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_1 = h_1 = 0.7616\n",
    "$$\n",
    "\n",
    "$$\n",
    "l_1 = \\tfrac12 y_1^2 \\approx 0.2908\n",
    "$$\n",
    "\n",
    "第二步：\n",
    "\n",
    "$$\n",
    "a_2 = 1 + h_1 = 1.7616,\\quad\n",
    "h_2 = \\tanh(1.7616)=0.9427\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 = h_2,\\quad\n",
    "l_2 = \\tfrac12 y_2^2 \\approx 0.4441\n",
    "$$\n",
    "\n",
    "总损失：\n",
    "\n",
    "$$\n",
    "L = l_1+l_2 \\approx 0.7349\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 输出层反向（从 (y_t) 回来）\n",
    "\n",
    "对平方误差：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_t}{\\partial y_t} = y_t - \\hat y_t\n",
    "$$\n",
    "\n",
    "因此：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_1}{\\partial h_1} = y_1 = 0.7616\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_2}{\\partial h_2} = y_2 = 0.9427\n",
    "$$\n",
    "\n",
    "记隐状态的“上游梯度”为：\n",
    "\n",
    "$$\n",
    "\\delta_t^h := \\frac{\\partial L}{\\partial h_t}\n",
    "$$\n",
    "\n",
    "因为 (h_2) 只影响 (l_2)：\n",
    "\n",
    "$$\n",
    "\\delta_2^h = 0.9427\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 BPTT 的递推：未来的梯度也要回传\n",
    "\n",
    "因为\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(a_t),\\qquad\n",
    "a_t = w_x x_t + w_h h_{t-1}\n",
    "$$\n",
    "\n",
    "有\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = 1 - h_t^2\n",
    "$$\n",
    "\n",
    "对于 (h_2)：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_2}\n",
    "= \\delta_2^h \\cdot (1-h_2^2)\n",
    "= 0.9427\\cdot 0.1113\n",
    "= 0.1050\n",
    "$$\n",
    "\n",
    "这会进一步传给 (h_1)：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_1}\\Big|_{\\text{来自 } t=2}\n",
    "= \\frac{\\partial L}{\\partial a_2}\\cdot w_h\n",
    "= 0.1050\n",
    "$$\n",
    "\n",
    "因此 (h_1) 的总梯度：\n",
    "\n",
    "$$\n",
    "\\delta_1^h\n",
    "= \\underbrace{\\frac{\\partial l_1}{\\partial h_1}}_{\\text{来自当前}}\n",
    "$$\n",
    "$$\n",
    " \\underbrace{\\frac{\\partial L}{\\partial h_1}|*{\\text{来自 } t=2}}\n",
    " $$\n",
    "$$\n",
    "{\\text{来自未来}}\n",
    "  = 0.7616 + 0.1050\n",
    "  = 0.8666\n",
    "$$\n",
    "\n",
    "这体现了 BPTT 的本质：\n",
    "\n",
    "> **某个隐状态的梯度 = 本时间步损失的贡献 + 所有未来时间步的贡献。**\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 最终参数梯度 = 时间求和\n",
    "\n",
    "对 (w_x)：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_x}\n",
    "= \\sum_t \\frac{\\partial L}{\\partial a_t}\\cdot \\frac{\\partial a_t}{\\partial w_x}\n",
    "= 0.3639 + 0.1050\n",
    "= 0.4689\n",
    "$$\n",
    "\n",
    "对 (w_h)：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_h}\n",
    "= \\sum_t \\frac{\\partial L}{\\partial a_t}\\cdot \\frac{\\partial a_t}{\\partial w_h}\n",
    "= 0 + 0.0799\n",
    "= 0.0799\n",
    "$$\n",
    "\n",
    "对 (w_y)（输出层参数）：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_y}\n",
    "= \\sum_t (y_t - \\hat y_t) h_t\n",
    "= 1.4687\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. BPTT 的统一递推公式（标量版）\n",
    "\n",
    "隐状态梯度递推：\n",
    "\n",
    "$$\n",
    "\\delta_t^h\n",
    "= (y_t - \\hat y_t) w_y\n",
    "$$\n",
    "$$\n",
    " \\delta_{t+1}^h \\cdot (1 - h_{t+1}^2) w_h\n",
    "  $$\n",
    "\n",
    "参数梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_x}\n",
    "= \\sum_t \\delta_t^h (1-h_t^2)x_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_h}\n",
    "= \\sum_t \\delta_t^h (1-h_t^2)h_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_y}\n",
    "= \\sum_t (y_t - \\hat y_t)h_t\n",
    "$$\n",
    "\n",
    "这些就是 RNN/BPTT 的基本推导。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 最重要的理解：\n",
    "\n",
    "### 对 (h_t) 求偏导不是因为要优化它，而是因为链式法则必须经过它\n",
    "\n",
    "**隐状态不是参数，但它是多条通往损失 L 的路径的“交叉节点”。\n",
    "为了让梯度继续往前传，我们必须知道它的上游梯度。**\n",
    "\n",
    "所以：\n",
    "\n",
    "* **反向传播会对所有中间变量求梯度**（包括 (h_t)、(a_t)、(y_t)）\n",
    "* **但只有参数的梯度（如 (w_x,w_h,w_y)）会被优化器使用**\n",
    "\n",
    "一句话总结：\n",
    "\n",
    "> **对 (h_t) 求偏导是为了“把梯度传过去”，\n",
    "> 而不是为了“更新 h_t”。\n",
    "> 它的角色是：链式法则中的中间站。**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
