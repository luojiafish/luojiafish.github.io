{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0e7d74",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "\n",
    "> 卷积，相比于全连接的优劣势\n",
    "\n",
    "> 感受野是相对与原始输入来说的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64e09b",
   "metadata": {},
   "source": [
    "### 卷积的演变\n",
    "> 自动控制原理中的卷积：  \n",
    "> **输出就是过去输入按某种权重 $h(\\cdot)$ 在时间上滑动叠加的结果，这个叠加就是卷积。**\n",
    "\n",
    "$$\n",
    "y(t) = x(t) * h(t)\n",
    "     = \\int_{-\\infty}^{+\\infty} x(\\tau)\\,h(t-\\tau)\\,d\\tau\n",
    "$$\n",
    "\n",
    "其中 $x(t)$ 是输入信号，$h(t)$ 是该线性时不变系统的单位冲激响应。\n",
    "\n",
    "> 线性时不变系统中，单位冲激响应就是传递函数的反拉普拉斯变换（一阶惯性系统为例）：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{-1}\\left\\{\\frac{1}{s + a}\\right\\} = e^{-at} u(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "G(s) = \\frac{1}{Ts + 1}\n",
    "      = \\frac{1}{T}\\cdot\\frac{1}{s + \\frac{1}{T}}\n",
    "\\Rightarrow\n",
    "h(t) = \\frac{1}{T} e^{-t/T} u(t)\n",
    "$$\n",
    "\n",
    "> 以一阶惯性系统对单位阶跃输入的卷积为例：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y(t) &= x(t) * h(t)\n",
    "     = \\int_{-\\infty}^{+\\infty} u(\\tau)\\,\\frac{1}{T} e^{-\\frac{(t-\\tau)}{T}} u(t-\\tau)\\,d\\tau \\\\\n",
    "     &= \\int_{0}^{t} \\frac{1}{T} e^{-\\frac{(t-\\tau)}{T}}\\,d\\tau\n",
    "      = 1 - e^{-t/T}, \\quad t \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> 也可以把卷积理解成“时间窗 + 加权叠加”的过程。  \n",
    "> 以一阶惯性系统接收一个宽度为 1 的方波输入为例：\n",
    "> 令\n",
    "$$\n",
    "x(t) = w(t) =\n",
    "\\begin{cases}\n",
    "1, & 0 \\le t \\le 1\\\\[2pt]\n",
    "0, & \\text{其他}\n",
    "\\end{cases}\n",
    "$$\n",
    "> 这个 $w(t)$ 本质上就是一个宽度为 1 的矩形窗（窗函数），表示“系统只在 $[0,1]$ 这一秒钟内接收到输入，其他时刻都为 0”。\n",
    "\n",
    "> 对这个输入的卷积为：\n",
    " $$\n",
    "y(t) = (w * h)(t)\n",
    "= \\int_{-\\infty}^{+\\infty} w(\\tau)\\,h(t-\\tau)\\,d\\tau\n",
    "$$\n",
    ">由于 $w(\\tau)$ 只在 $[0,1]$ 非零，积分会被“窗”截断：\n",
    "> - 当 $0 \\le t \\le 1$ 时：\n",
    "$$\n",
    " y(t) = \\int_0^t \\frac{1}{T} e^{-\\frac{(t-\\tau)}{T}}\\,d\\tau\n",
    "      = 1 - e^{-t/T}\n",
    " $$\n",
    ">   系统正在“积累”这 1 秒内持续输入的影响；\n",
    "> - 当 $t > 1$ 时：\n",
    " $$\n",
    " y(t) = \\int_0^1 \\frac{1}{T} e^{-\\frac{(t-\\tau)}{T}}\\,d\\tau\n",
    "      = (e^{1/T}-1)\\,e^{-t/T}\n",
    " $$\n",
    ">   输入已经关掉（窗外为 0），输出只剩下系统对这 1 秒输入的“余辉”，按 $h(t)$ 的形状缓慢衰减。\n",
    "\n",
    "> 从这个角度看，窗函数 $w(t)$ 负责“挑出”某一段时间内的输入，再由卷积核 $h(\\cdot)$ 对这段历史进行加权叠加，这和深度学习中卷积核在时间或空间上滑动，对一个局部窗内的像素（或特征）做加权求和，在本质上是同一个思想。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7bf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "# 2d卷积的实现\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = nd.zeros((X.shape[0]-h+1, X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range( Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+h, j:j+w]*K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15766a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[19. 25.]\n",
       " [37. 43.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = nd.array([[0,1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdbb6c",
   "metadata": {},
   "source": [
    "### 边缘提取最基础的小例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "587acce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个核\n",
    "K = nd.array([[1, -1]])\n",
    "# 假设一个图像\n",
    "X = nd.ones((6, 6))\n",
    "X[:, 2:4] = 0\n",
    "# 边缘提取，相同的为0，不相同的为非0\n",
    "# 有效的表征局部特征\n",
    "Y = corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c3716af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 0.401\n",
      "batch 4, loss 0.383\n",
      "batch 6, loss 0.365\n",
      "batch 8, loss 0.347\n",
      "batch 10, loss 0.331\n",
      "batch 12, loss 0.315\n",
      "batch 14, loss 0.300\n",
      "batch 16, loss 0.286\n",
      "batch 18, loss 0.273\n",
      "batch 20, loss 0.260\n",
      "batch 22, loss 0.248\n",
      "batch 24, loss 0.236\n",
      "batch 26, loss 0.225\n",
      "batch 28, loss 0.214\n",
      "batch 30, loss 0.204\n",
      "batch 32, loss 0.195\n",
      "batch 34, loss 0.185\n",
      "batch 36, loss 0.177\n",
      "batch 38, loss 0.168\n",
      "batch 40, loss 0.160\n",
      "batch 42, loss 0.153\n",
      "batch 44, loss 0.146\n",
      "batch 46, loss 0.139\n",
      "batch 48, loss 0.132\n",
      "batch 50, loss 0.126\n",
      "batch 52, loss 0.120\n",
      "batch 54, loss 0.114\n",
      "batch 56, loss 0.109\n",
      "batch 58, loss 0.104\n",
      "batch 60, loss 0.099\n",
      "batch 62, loss 0.094\n",
      "batch 64, loss 0.090\n",
      "batch 66, loss 0.086\n",
      "batch 68, loss 0.082\n",
      "batch 70, loss 0.078\n",
      "batch 72, loss 0.074\n",
      "batch 74, loss 0.071\n",
      "batch 76, loss 0.067\n",
      "batch 78, loss 0.064\n",
      "batch 80, loss 0.061\n",
      "batch 82, loss 0.058\n",
      "batch 84, loss 0.055\n",
      "batch 86, loss 0.053\n",
      "batch 88, loss 0.050\n",
      "batch 90, loss 0.048\n",
      "batch 92, loss 0.046\n",
      "batch 94, loss 0.044\n",
      "batch 96, loss 0.041\n",
      "batch 98, loss 0.040\n",
      "batch 100, loss 0.038\n"
     ]
    }
   ],
   "source": [
    "# 简单训练一个卷积层，实际就是在训练卷积核参数\n",
    "conv2d = nn.Conv2D(1,kernel_size=(1, 2))\n",
    "conv2d.initialize()\n",
    "# reshape = 不改数据，只改“你怎么切片看它”的方式\n",
    "X = X.reshape((1, 1, 6, 6))\n",
    "Y = Y.reshape((1, 1, 6, 5))\n",
    "\n",
    "epoch = 100\n",
    "# for i in range(epoch):\n",
    "#     with autograd.record():\n",
    "#         y_hat = conv2d(X)\n",
    "#         l = (y_hat - Y)**2\n",
    "#     l.backward()\n",
    "#     conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\n",
    "#     if (i + 1) % 2 == 0:\n",
    "#         print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))\n",
    "for i in range(epoch):\n",
    "    with autograd.record():\n",
    "        y_hat = conv2d(X)\n",
    "        l = ((y_hat - Y)**2).mean()\n",
    "    l.backward()\n",
    "    conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\n",
    "    conv2d.bias.data()[:] -= 3e-2 * conv2d.bias.grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb0815cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.697463  -0.6961364]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data().reshape((1, 2))\n",
    "#conv2d.bias.data().reshape((1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ee9df",
   "metadata": {},
   "source": [
    "### 填充和步幅\n",
    "> 当核为大小为偶数时，如何实现两侧填充不一样\n",
    "padding（0，1）默认是两侧填充一样，不过是高两侧填充0，宽填充1\n",
    "### 通道\n",
    "> 我有 3 通道输入，想变成 2 通道输出，那么有 2 个 3*h*w 形状的核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # zip 的行为 和 对 NDArray 迭代时默认按第 0 维切片\n",
    "    # 在in 后面的都是可迭代对象\n",
    "    # []变成列表推导式\n",
    "    # * 是把列表拆开当作多个参数传进去，相当于：\n",
    "    # Ys = [d2l.corr2d(x, k) for x, k in zip(X, K)]\n",
    "    # nd.add_n(Ys[0], Ys[1], Ys[2], ...)\n",
    "    return nd.add_n(*[d2l.corr2d(x, k) for x, k in zip(X, K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97aa19",
   "metadata": {},
   "source": [
    "## 1×1 卷积（1×1 卷积 = 对每个像素做一次 C_in → C_out 的全连接）：\n",
    "矩阵视角（3×H×W → 2×H×W）\n",
    "\n",
    "设 $N = H \\times W$。\n",
    "\n",
    "### 1. 展平空间维度\n",
    "\n",
    "输入：\n",
    "$$\n",
    "X \\in \\mathbb{R}^{3 \\times H \\times W}\n",
    "$$\n",
    "\n",
    "展平为：\n",
    "$$\n",
    "X' \\in \\mathbb{R}^{3 \\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,N} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,N} \\\\\n",
    "x_{3,1} & x_{3,2} & \\cdots & x_{3,N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 1×1 卷积核（2 个输出通道）\n",
    "\n",
    "$$\n",
    "W \\in \\mathbb{R}^{2 \\times 3},\\quad\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 矩阵乘法（对每个样本做 3→2 变换）\n",
    "\n",
    "$$\n",
    "Y' = W X' \\in \\mathbb{R}^{2 \\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y' =\n",
    "\\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} & \\cdots & y_{1,N} \\\\\n",
    "y_{2,1} & y_{2,2} & \\cdots & y_{2,N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 恢复空间形状\n",
    "\n",
    "$$\n",
    "Y' \\Rightarrow Y \\in \\mathbb{R}^{2 \\times H \\times W}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
