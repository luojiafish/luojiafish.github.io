{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6a3361",
   "metadata": {},
   "source": [
    "# 深度学习中的矩阵运算统一总结\n",
    "\n",
    "（Linear / MLP / CNN / RNN 对比）\n",
    "\n",
    "本总结统一解释不同网络结构中矩阵乘法的本质，包括：\n",
    "\n",
    "* 为什么线性回归不需要滑动窗口\n",
    "* 为什么 CNN 需要 `corr2d`\n",
    "* 为什么 RNN 可以把 `[T, bs, d_in]` 看成 T 次 `[bs, d_in]`\n",
    "* 各网络的本质都是“对很多向量重复使用同一个 W”\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 线性回归 / MLP：一次性矩阵乘法\n",
    "\n",
    "**输入：**\n",
    "\n",
    "```\n",
    "X : [bs, n_features]\n",
    "```\n",
    "\n",
    "**权重：**\n",
    "\n",
    "```\n",
    "W : [n_features, out_dim]\n",
    "```\n",
    "\n",
    "**输出：**\n",
    "\n",
    "$$\n",
    "Y = XW + b\n",
    "$$\n",
    "\n",
    "特点：\n",
    "\n",
    "* 整个输入是一个完整的特征向量\n",
    "* 只做一次线性变换\n",
    "* 不涉及空间或序列结构\n",
    "* 不需要滑动窗口\n",
    "\n",
    "---\n",
    "\n",
    "# 2. CNN：在空间维上重复做局部线性变换\n",
    "\n",
    "**输入：**\n",
    "\n",
    "```\n",
    "X : [bs, c_in, h, w]\n",
    "```\n",
    "\n",
    "**卷积核：**\n",
    "\n",
    "```\n",
    "K : [c_out, c_in, k, k]\n",
    "```\n",
    "\n",
    "卷积是：\n",
    "\n",
    "> 对每个位置 (i,j)，从输入取一个局部 patch `[c_in, k, k]`，展平为向量后，乘以权重矩阵 `W`。\n",
    "\n",
    "数学表达（单通道示例）：\n",
    "\n",
    "$$\n",
    "Y_{i,j} = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} X_{i+u, j+v} \\cdot K_{u,v}\n",
    "$$\n",
    "\n",
    "向量化（patch → vec）后：\n",
    "\n",
    "$$\n",
    "Y_{i,j} = \\text{vec}(X_{i:i+k,j:j+k}) \\cdot \\text{vec}(K)\n",
    "$$\n",
    "\n",
    "矩阵实现（im2col）：\n",
    "\n",
    "```\n",
    "[bs, c_in, h, w]\n",
    "    → patches : [bs, hw_out, c_in*k*k]\n",
    "    → matmul  : [bs, hw_out, c_out]\n",
    "    → reshape : [bs, c_out, h_out, w_out]\n",
    "```\n",
    "\n",
    "**为什么比 MLP 多一步？**\n",
    "因为 CNN 需要“滑动窗口”去获得局部 patch，而 MLP 不处理空间结构。\n",
    "\n",
    "---\n",
    "\n",
    "# 3. RNN：在时间维上重复做线性变换\n",
    "\n",
    "**输入：**\n",
    "\n",
    "```\n",
    "X : [T, bs, d_in]\n",
    "```\n",
    "\n",
    "RNN 单步公式：\n",
    "\n",
    "$$\n",
    "h_t = \\phi ( X_t W_{xh} + h_{t-1} W_{hh} + b_h )\n",
    "$$\n",
    "\n",
    "按时间拆开：\n",
    "\n",
    "```python\n",
    "for t in range(T):\n",
    "    x_t = X[t]            # [bs, d_in]\n",
    "    h_t = x_t @ W_xh + h_{t-1} @ W_hh\n",
    "```\n",
    "\n",
    "输出层：\n",
    "\n",
    "$$\n",
    "o_t = h_t W_{hy} + b_y\n",
    "$$\n",
    "\n",
    "本质：\n",
    "\n",
    "* 与 MLP 相同，输入都是 `[bs, d_in]`\n",
    "* RNN 只是在 **时间维 T 上重复** 应用线性变换\n",
    "* 同一套参数在每个时间步共享\n",
    "\n",
    "---\n",
    "\n",
    "# 4. 各网络的“重复作用维度”对比\n",
    "\n",
    "所有深度学习网络都可以抽象为：\n",
    "\n",
    "> **把输入拆成多个向量，对每个向量应用同一个线性变换 W**\n",
    "\n",
    "| 模型类型       | 输入形状            | 重复操作的维度                 | 小向量是什么？        |\n",
    "| ---------- | --------------- | ----------------------- | -------------- |\n",
    "| 线性回归 / MLP | `[bs, n]`       | batch (`bs`)            | 整个特征向量         |\n",
    "| CNN        | `[bs, c, h, w]` | batch × 空间 (`bs * h*w`) | 每个像素的感受野 patch |\n",
    "| RNN        | `[T, bs, d]`    | batch × 时间 (`T * bs`)   | 每个时间步的输入向量     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
