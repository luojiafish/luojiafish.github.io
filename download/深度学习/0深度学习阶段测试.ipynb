{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9cf9e9",
   "metadata": {},
   "source": [
    "下面是一份你可以直接放到 **Jupyter Notebook 的 Markdown 单元**里的“阶段测试 + 复习提纲”。\n",
    "内容默认你已经学到：前面基础 + MLP + CNN + BatchNorm + ResNet + RNN + BPTT + GRU/LSTM（大概对应《动手学深度学习》前几章 + RNN 章节的核心）。\n",
    "\n",
    "你可以新建一个 `rnn_review_test.ipynb`，第一个 cell 选 Markdown，把下面整段贴进去即可。\n",
    "\n",
    "---\n",
    "\n",
    "# 深度学习阶段测试（至 RNN / GRU / LSTM）\n",
    "\n",
    "（自测 + 复习用）\n",
    "\n",
    "---\n",
    "\n",
    "## 0. 使用说明\n",
    "\n",
    "* 本测试分为：\n",
    "\n",
    "  * 理解题（概念 & 推导）\n",
    "  * 计算 / 推导题（手算为主）\n",
    "  * 编程题（建议用 MXNet/PyTorch 任意一个）\n",
    "* **建议做法：**\n",
    "\n",
    "  1. 先在草稿纸上写出解答/推导；\n",
    "  2. 再在 Notebook 里整理一版“能给别人看的答案”；\n",
    "  3. 完成后对照“第 5 节：应达到的能力要求”自查。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 学习目标（你应该掌握的东西）\n",
    "\n",
    "完成本阶段后，你至少应该做到：\n",
    "\n",
    "1. **基础神经网络与训练：**\n",
    "\n",
    "   * 说清楚：前向传播、损失函数、反向传播、参数更新（SGD）基本流程；\n",
    "   * 能写出简单线性回归 / softmax 分类的损失和梯度（至少知道形式和意义）。\n",
    "2. **多层感知机（MLP）：**\n",
    "\n",
    "   * 知道为什么要用非线性激活（ReLU/tanh 等）；\n",
    "   * 理解过拟合 / 欠拟合、参数量、深度对表达能力的影响。\n",
    "3. **卷积神经网络（CNN）：**\n",
    "\n",
    "   * 理解卷积核、步幅、填充、池化的作用；\n",
    "   * 会根据输入尺寸、卷积参数，手算输出尺寸。\n",
    "4. **批量归一化（BatchNorm）与残差网络（ResNet）：**\n",
    "\n",
    "   * 说清 BatchNorm 做了什么、为什么有助于训练；\n",
    "   * 理解 ResNet 的残差连接是怎么缓解梯度消失、为什么更容易优化。\n",
    "5. **RNN / BPTT：**\n",
    "\n",
    "   * 画出一个简单 RNN 的计算图（时间展开）；\n",
    "   * 能用链式法则解释什么是“通过时间反向传播（BPTT）”，\n",
    "     知道为什么会出现梯度爆炸 / 消失。\n",
    "6. **GRU / LSTM：**\n",
    "\n",
    "   * 理解 GRU 中 update gate / reset gate 的含义，能读懂公式；\n",
    "   * 理解 LSTM 的 cell state、input gate、forget gate、output gate 各自干什么；\n",
    "   * 能说清 GRU vs LSTM 在结构上的主要差异与权衡。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 概念理解题（简答）\n",
    "\n",
    "> 建议：每题尽量用 **3–6 句话**说清楚，重点写“**为什么**”，不要只写结论。\n",
    "\n",
    "### 2.1 训练过程 & 反向传播\n",
    "\n",
    "1. 请用自己的话描述一次 **神经网络训练的完整流程**，从“给定一小批训练样本（mini-batch）”开始，到“更新参数”结束，中间关键步骤要写清楚。\n",
    "2. 什么是 **反向传播（backpropagation）**？它和链式法则之间是什么关系？\n",
    "3. 为什么一般不会“对每一层单独设计优化器”，而是统一用一个优化算法（比如 SGD/Adam）更新所有参数？\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 多层感知机（MLP）\n",
    "\n",
    "1. 为什么如果所有层都是线性变换（没有激活函数），即使用很多层，模型表达能力仍然等价于一层线性变换？\n",
    "2. 对于分类任务，softmax + 交叉熵损失相比于 MSE（平方误差）更常用，简要说明一个主要原因。\n",
    "3. 简要解释什么是 **欠拟合** 和 **过拟合**，分别在什么情况下出现？\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 卷积神经网络（CNN）\n",
    "\n",
    "1. 和全连接层相比，卷积层在参数共享和感受野方面有什么特点？这两点分别带来什么好处？\n",
    "2. 池化层（max/avg pooling）的主要作用是什么？如果完全去掉池化层，会有什么影响（可以从参数量、特征图尺寸、平移不变性等角度说明）？\n",
    "3. 给定输入特征图大小为 $$H \\times W$$，卷积核大小 $$k \\times k$$，步幅为 $$s$$，padding 为 $$p$$，请写出输出特征图的高度 $$H'$$ 和宽度 $$W'$$ 的计算公式。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 BatchNorm 与 ResNet\n",
    "\n",
    "1. BatchNorm 在训练时，大致做了哪两步操作？（提示：标准化 + 仿射变换）\n",
    "2. 简要说明：**为什么 BatchNorm 有助于训练深层网络？** 可以从梯度稳定、对学习率的影响等角度说明。\n",
    "3. 残差块（Residual Block）相比普通堆叠层，主要多了一条什么路径？\n",
    "   这条路径如何帮助缓解“退化问题”（更深反而更难训）？\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 RNN / BPTT\n",
    "\n",
    "1. 写出一个简单 RNN 的递推公式（标量或向量形式都可以），并解释各项含义（输入、隐藏状态、权重、激活函数）。\n",
    "2. 用自己的话解释什么是 **“通过时间反向传播（BPTT）”**。\n",
    "   可以从“把 RNN 在时间上展开成一个深层网络，然后……”开始描述。\n",
    "3. 简要说明：RNN 中为什么容易出现 **梯度消失** 或 **梯度爆炸**？\n",
    "   可以结合“梯度在时间上不断相乘”来回答。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 GRU 与 LSTM\n",
    "\n",
    "1. 写出 GRU 的三个核心公式（update gate、reset gate、候选隐藏状态 + 最终隐藏状态），并用一句话解释每个门的作用。\n",
    "2. LSTM 中的记忆细胞 $$c_t$$ 与隐藏状态 $$h_t$$ 有什么区别？\n",
    "   为什么要把“记忆”和“输出表征”拆成两个量？\n",
    "3. 比较 GRU 与 LSTM 的主要结构差异：\n",
    "\n",
    "   * 它们各自有多少个门？\n",
    "   * 哪些门是互补关系，哪些门是相互独立的？\n",
    "   * 哪一个结构更紧凑？哪一个表达更灵活？\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 计算 / 推导题（手算为主）\n",
    "\n",
    "### 3.1 卷积输出尺寸手算\n",
    "\n",
    "**题 1**\n",
    "\n",
    "输入特征图大小 $$32 \\times 32$$，通道数 3；\n",
    "卷积层参数：卷积核大小 $$3\\times 3$$，padding = 1，stride = 2，输出通道数为 16。\n",
    "\n",
    "1. 请计算该卷积层输出特征图的空间尺寸（高度/宽度）。\n",
    "2. 该卷积层总共有多少个可学习参数？（忽略偏置或说明你是否包含）\n",
    "\n",
    "> 提示：如果采用常见的“same padding”形式，padding=1 时 $$3\\times 3$$ 卷积在空间上不会改变大小（stride=1 的时候），这里 stride=2 会减半。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 一个简单全连接网络的梯度（单样本）\n",
    "\n",
    "考虑一个两层全连接网络（标量输入，标量输出）：\n",
    "\n",
    "* 输入 $$x$$（标量）\n",
    "* 隐藏层：\n",
    "\n",
    "  $$\n",
    "  h = \\tanh(w_1 x + b_1)\n",
    "  $$\n",
    "* 输出层：\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = w_2 h + b_2\n",
    "  $$\n",
    "* 损失函数为均方误差：\n",
    "\n",
    "  $$\n",
    "  L = \\frac{1}{2} (\\hat{y} - y)^2\n",
    "  $$\n",
    "\n",
    "请推导：\n",
    "\n",
    "1. $$\\frac{\\partial L}{\\partial w_2},\\ \\frac{\\partial L}{\\partial b_2}$$\n",
    "2. $$\\frac{\\partial L}{\\partial w_1},\\ \\frac{\\partial L}{\\partial b_1}$$\n",
    "\n",
    "要求写出完整链式法则的中间步骤（至少写出每一步的“谁对谁的导数”）。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 标量 RNN 的 BPTT 推导（长度为 2 的序列）\n",
    "\n",
    "考虑我们之前讨论过的标量 RNN：\n",
    "\n",
    "* 初始状态：$$h_0 = 0$$\n",
    "* 递推：\n",
    "\n",
    "  $$\n",
    "  h_t = \\tanh(w_x x_t + w_h h_{t-1}), \\\n",
    "  y_t = w_y h_t, \\\n",
    "  l_t = \\frac12 (y_t - \\hat{y}_t)^2, \\\n",
    "  L = l_1 + l_2\n",
    "  $$\n",
    "\n",
    "请在**不代具体数值**的情况下推导出：\n",
    "\n",
    "1. 输出层梯度：\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_y}\n",
    "   $$\n",
    "   用 $$h_1, h_2, y_1, y_2, \\hat{y}_1, \\hat{y}_2$$ 表示结果。\n",
    "2. 定义：\n",
    "   $$\n",
    "   \\delta_t^h := \\frac{\\partial L}{\\partial h_t}\n",
    "   $$\n",
    "   请写出 $$\\delta_2^h$$ 与 $$\\delta_1^h$$ 的表达式，用链式法则体现出“来自当前步 + 来自未来步”的结构。\n",
    "3. 在此基础上，写出 $$\\frac{\\partial L}{\\partial w_x}$$ 和 $$\\frac{\\partial L}{\\partial w_h}$$ 的通用形式（用求和符号表示）。\n",
    "\n",
    "> 要求：推导过程中尽量显式写出“谁对谁的导数”，比如\n",
    "> $$\\frac{\\partial L}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial a_t}$$ 等。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 梯度消失的简单分析（可选思考）\n",
    "\n",
    "考虑一个没有激活函数的简化 RNN：\n",
    "\n",
    "$$\n",
    "h_t = w h_{t-1}, \\quad h_0 = 1\n",
    "$$\n",
    "\n",
    "假设损失只依赖于 $$h_T$$，即 $$L = \\frac12 h_T^2$$。\n",
    "\n",
    "1. 写出 $$h_T$$ 关于 $$w$$ 的显式表达式。\n",
    "2. 推导 $$\\frac{\\partial L}{\\partial w}$$，看看里面出现了多少次 $$w$$。\n",
    "3. 分别讨论：\n",
    "\n",
    "   * 当 $$|w| < 1$$ 时，$$\\frac{\\partial L}{\\partial w}$$ 随 $$T$$ 的变化趋势；\n",
    "   * 当 $$|w| > 1$$ 时，趋势又如何？\n",
    "4. 用一句话总结这和 “RNN 梯度消失 / 爆炸” 的关系。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 编程题（Jupyter Notebook 中完成）\n",
    "\n",
    "> 框架任选：MXNet / PyTorch / TensorFlow 均可。\n",
    "> 建议尽量自己手写核心部分（例如 RNNCell/GRUCell），不要完全依赖现成高级 API。\n",
    "\n",
    "### 4.1 手写一个简单 RNNCell 并做序列分类（简版）\n",
    "\n",
    "**任务描述：**\n",
    "\n",
    "* 构造一个简单的“玩具数据集”：输入是长度为 $$T$$ 的二进制序列（0/1），输出是一个二分类标签，比如：\n",
    "\n",
    "  * 序列中 1 的个数是偶数 → 类 0\n",
    "  * 1 的个数是奇数 → 类 1\n",
    "* 使用一个简单 RNN 对该任务进行学习。\n",
    "\n",
    "**要求：**\n",
    "\n",
    "1. 手写一个 RNNCell：\n",
    "\n",
    "   ```python\n",
    "   class MyRNNCell(nn.Block):  # 或 nn.Module\n",
    "       def __init__(self, ...):\n",
    "           ...\n",
    "       def forward(self, x_t, h_prev):\n",
    "           # x_t: (batch_size, input_dim)\n",
    "           # h_prev: (batch_size, hidden_dim)\n",
    "           # return h_t\n",
    "   ```\n",
    "2. 写一个循环，把长度为 T 的序列喂给 RNNCell，在最后一个时间步的隐藏状态上接一层全连接进行分类。\n",
    "3. 训练若干 epoch，观察在训练集/验证集上的准确率，看看能否学会这个“奇偶计数”的任务。\n",
    "4. 在 Notebook 中简单记录：\n",
    "\n",
    "   * 模型结构；\n",
    "   * 训练曲线（loss/accuracy）；\n",
    "   * 你观察到的现象（例如 T 变长后是否更难学？）。\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 对比 GRU / LSTM 与普通 RNN 在长序列上的表现（开放题）\n",
    "\n",
    "> 如果时间不够，可以先跳过，只做 4.1。\n",
    "\n",
    "**任务建议：**\n",
    "\n",
    "* 使用框架自带的 GRU / LSTM 实现；\n",
    "* 保持隐藏维度、层数尽量一致，控制变量；\n",
    "* 构造一个相对长一点的序列任务，比如：\n",
    "\n",
    "  * 从序列开头某个位置给定一个“标记 bit”，模型在最后一个时间步必须输出这个 bit；\n",
    "  * 或简单的字符级语言模型 / next-step prediction。\n",
    "\n",
    "**要求（可以写成实验报告小节）：**\n",
    "\n",
    "1. 描述数据集与任务。\n",
    "2. 分别用：\n",
    "\n",
    "   * 普通 RNN\n",
    "   * GRU\n",
    "   * LSTM\n",
    "     训练模型，并记录训练过程中 loss/accuracy 的变化。\n",
    "3. 对比这三者在：\n",
    "\n",
    "   * 收敛速度；\n",
    "   * 能否成功学会长距离依赖\n",
    "     上的差异。\n",
    "4. 用你自己的话总结：**从结果上，你怎么看待“GRU/LSTM 更擅长处理长依赖”的说法？**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 自查：你应该达到的要求（检查清单）\n",
    "\n",
    "做完上面的题之后，可以用下面这张清单自查：\n",
    "\n",
    "### 5.1 概念层面\n",
    "\n",
    "* [ ] 能清晰描述一次“前向 + 反向 + 更新”训练流水线；\n",
    "* [ ] 知道为什么 MLP 必须有非线性激活；\n",
    "* [ ] 能解释卷积层的感受野、参数共享、输出尺寸计算；\n",
    "* [ ] 能说出 BatchNorm 做了什么，以及对训练有什么帮助；\n",
    "* [ ] 知道 ResNet 为什么在实践中比“纯堆叠网络”更容易训练；\n",
    "* [ ] 能画出一个简单 RNN / GRU / LSTM 的计算图并讲明各门的作用；\n",
    "* [ ] 能解释 RNN 梯度消失 / 爆炸的本质原因。\n",
    "\n",
    "### 5.2 推导 / 计算层面\n",
    "\n",
    "* [ ] 能手算简单 CNN 的输出尺寸和参数量；\n",
    "* [ ] 能用链式法则手推一层或两层的小网络的梯度；\n",
    "* [ ] 能按照标量形式，写出长度为 2 的 RNN 的 BPTT 推导（至少知道关键步骤）；\n",
    "* [ ] 能用极简 RNN $$h_t = w h_{t-1}$$ 的例子说明梯度消失 / 爆炸。\n",
    "\n",
    "### 5.3 编程 / 实践层面\n",
    "\n",
    "* [ ] 能在 Jupyter Notebook 里搭一个简单的分类/回归模型，完成训练和测试；\n",
    "* [ ] 能手写一个 RNNCell（或用框架 API）做一个小序列任务；\n",
    "* [ ] 大致知道如何在代码里切换 RNN / GRU / LSTM，并观察训练效果差异。\n",
    "\n",
    "---\n",
    "\n",
    "> 如果你做完这份测试，绝大部分地方都能写出自己的理解和推导，那么你这一阶段的“基础 + 底层直觉”已经很好了。以后遇到更复杂的模型（Transformer、各种 fancy 的模块），都能用现在这套“先看计算图，再想梯度怎么走”的视角去拆解。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
